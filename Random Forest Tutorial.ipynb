{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Random Forest in Python\n",
    "\n",
    "In this notebook, we will implement a random forest in Python. With machine learning in Python, it's very easy to build a complex model without having any idea how it works. Therefore, we'll start with a single decision tree and a simple problem, and then work our way to a random forest and a real-world problem. \n",
    "\n",
    "Once we understand how a single decision tree thinks, we can transfer this knowledge to an entire forest of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed to ensure reproducible runs\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Simple: Basic Problem\n",
    "\n",
    "To begin, we'll use a very simple problems with only two features and two classes. This is a binary classification problem. \n",
    "\n",
    "First, we create the features `X` and the labels `y`. There are only two features, which will allow us to visualize the data and which makes this a very easy problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 2], \n",
    "              [2, 1],\n",
    "              [2, 3], \n",
    "              [1, 2], \n",
    "              [1, 1],\n",
    "              [3, 3]])\n",
    "\n",
    "y = np.array([0, 1, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "To get a sense of the data, we can graph the data points with the number showing the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Data')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot formatting\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Plot each point as the label\n",
    "for x1, x2, label in zip(X[:, 0], X[:, 1], y):\n",
    "    plt.text(x1, x2, str(label), fontsize = 40, color = 'g',\n",
    "             ha='center', va='center')\n",
    "    \n",
    "# Plot formatting\n",
    "plt.grid(None);\n",
    "plt.xlim((0, 3.5));\n",
    "plt.ylim((0, 3.5));\n",
    "plt.xlabel('x1', size = 20); plt.ylabel('x2', size = 20); plt.title('Data', size = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there are only two features, this is a linearly inseparable problem. A simple linear classifier will not be able to draw a boundary that separates the classes. The single decision tree will be able to completely separate the points because it essentially draws many repeated linear boundaries between points. A decision tree is a non-parametric model because the number of parameters grows with the size of the data.\n",
    "\n",
    "## Single Decision Tree\n",
    "\n",
    "Here we quickly build and train a single decision tree on the data using Scikit-Learn. The tree will learn how to separate the points, building a flowchart of questions based on the feature values and the labels. At each stage, the decision tree makes splits by maximizing the reduction in Gini impurity. \n",
    "\n",
    "We'll use the default hyperparameters for the decision tree which means it can grow as deep as necessary in order to completely separate the classes. This will lead to overfitting because the model memorizes the training data, and in practice, we usually want to limit the depth of the tree so it can generalize to testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Make a decision tree and train\n",
    "tree = DecisionTreeClassifier(random_state=RSEED)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree has 9 nodes with maximum depth 3.\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision tree formed 9 nodes and reached a maximum depth of 3. It will have achieved 100% accuracy on the training data because we did not limit the depth and it therefore can classify every _training_ point perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Accuracy: {tree.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Tree\n",
    "\n",
    "To get a sense of how the decision tree \"thinks\", it's helpful to visualize the entire structure. This will show each node in the tree which we can use to make new predictions. Because the tree is relatively small, we can understand the entire image.\n",
    "\n",
    "First we export the tree as a `dot` file making sure to label the features and the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Export as dot\n",
    "export_graphviz(tree, 'tree.dot', rounded = True, \n",
    "                feature_names = ['x1', 'x2'], \n",
    "                class_names = ['0', '1'], filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use a system command and the Graphziv `dot` function to convert to a `png` (image). This requires Graphviz to be installed on your computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5654e4ac2da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Convert to png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-Tpng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree.dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-Gdpi=400'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \"\"\"\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    852\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1700\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'"
     ]
    }
   ],
   "source": [
    "from subprocess import call\n",
    "# Convert to png\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=400']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display the entire tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is an intuitive model: it makes decisions much as we might when faced with a problem by constructing a flowchart of questions. For each of the nodes (except the leaf nodes), the five rows represent:\n",
    "\n",
    "1. Question asked about the data based on a feature: this determines the way we traverse down the tree for a new datapoint.\n",
    "2. `gini`: the Gini Impurity of the node. The average (weighted by samples) gini impurity decreases with each level of the tree.\n",
    "3. `samples`: number of training observations in the node\n",
    "4. `value`: [number of samples in the first class, number of samples in the second class]\n",
    "5. `class`: the class predicted for all the points in the node if the tree ended at this depth (defaults to 0 for a tie).\n",
    "\n",
    "The leaf nodes (the terminal nodes at each branch) do not have a question because they are where the tree makes a prediction. All of the samples in a leaf node are assigned the same class. \n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "The Gini Impurity represents the probability that a randomly selected sample from the node will be incorrectly classified according to the distribution of samples in the node. At the top, there is a 44.4% chance that a randomly selected point would be incorrectly classified. The Gini Impurity is how the decision tree makes splits. It splits the samples based on the value of a feature that reduces the Gini Impurity by the largest amount. \n",
    "If we do the math, the average (weighted by number of samples) Gini Impurity decreases as we move down the tree. \n",
    "\n",
    "Eventually, the average Gini Impurity goes to 0.0 as we correctly classify each point. However, correctly classifying every single training point is usually not a good indicator because that means the model will not be able to generalize to the testing data! This model correclty classifies every single training point because we did not limit the maximum depth and during training, we give the model the answers as well as the features.\n",
    "\n",
    "### Limit Maximum Depth\n",
    "\n",
    "In practice, we usually want to limit the maximum depth of the decision tree (even in a random forest) so the tree can generalize better to testing data. Although this will lead to reduced accuracy on the training data, it can improve performance on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit maximum depth and train\n",
    "short_tree = DecisionTreeClassifier(max_depth = 2, random_state=RSEED)\n",
    "short_tree.fit(X, y)\n",
    "\n",
    "print(f'Model Accuracy: {short_tree.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same as before, visualizing the entire decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as dot\n",
    "export_graphviz(short_tree, 'shorttree.dot', rounded = True, \n",
    "                feature_names = ['x1', 'x2'], \n",
    "                class_names = ['0', '1'], filled = True)\n",
    "\n",
    "call(['dot', '-Tpng', 'shorttree.dot', '-o', 'shorttree.png', '-Gdpi=400']);\n",
    "Image('shorttree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model no longer gets perfect accuracy on the _training data_. However, it probably would do better on the _testing data_ since we have limited the maximum depth to prevent overfitting. This is an example of the bias - variance tradeoff in machine learning. A model with high variance has learned the training data very well but often cannot generalize to new points in the test set. On the other hand, a model with high bias has not learned the training data very well because it does not have enough complexity. This model will also not perform well on new points.\n",
    "\n",
    "Limiting the depth of a single decision tree is one way we can try to make a less biased model. Another option is to use an entire forest of trees, training each one on a random subsample of the training data. The final model then takes an average of all the individual decision trees to arrive at a classification. This is the idea behind the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this simple example has given you an idea of how a Decision Tree makes classifications. It looks at the features and the labels, and tries to construct a flowchart of questions that end in the correct classification for each label. If we don't limit the depth of the tree, it can correctly classify every single point in the training data. This will lead to overfitting though and an inability to do well on testing data. We didn't have any testing data in this example, but in the next problem, using a real-world dataset, we do and we'll see how overfitting can be an issue! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Dataset\n",
    "\n",
    "[Available Here](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system)\n",
    "\n",
    "The following data set is from the Centers for Disease Control and Prevention (CDC) and includes socioeconomic and lifestyle indicators for hundreds of thousands of individuals. The objective is to predict the overall health of an individual: either 0 for poor health or 1 for good health. We'll limit the data to 100,000 individuals to speed up training. \n",
    "\n",
    "The problem is imbalanced (far more of one label than another) so for assessing performance, we'll use recall, precision, receiver operating characteristic area under the curve (ROC AUC), and also plot the ROC curve. Accuracy is not a useful metric when dealing with an imbalanced problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning \n",
    "\n",
    "We'll read the data in and do a little cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2015_health.csv').sample(100000, random_state = RSEED)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_RFHLTH'] = df['_RFHLTH'].replace({2: 0})\n",
    "df = df.loc[df['_RFHLTH'].isin([0, 1])].copy()\n",
    "df = df.rename(columns = {'_RFHLTH': 'label'})\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label imbalanced means that accuracy is not the best metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't do any data exploration in this notebook, but in general, exploring the data is a best practice. This can help you for feature engineering (which we also won't do here) or by identifying and correcting anomalies / mistakes in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we drop a number of columns that we should not use for modeling (they are different versions of the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with missing values\n",
    "df = df.drop(columns = ['POORHLTH', 'PHYSHLTH', 'GENHLTH', 'PAINACT2', \n",
    "                        'QLMENTL2', 'QLSTRES2', 'QLHLTH2', 'HLTHPLN1', 'MENTHLTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Set\n",
    "\n",
    "To assess our predictions, we'll need to use a training and a testing set. The model learns from the training data and then makes predictions on the testing data. Since we have the correct answers for the testing data, we can tell how well the model is able to generalize to new data. It's important to only use the testing set once, because this is meant to be an estimate of how well the model will perform on new data. \n",
    "\n",
    "We'll save 30% of the examples for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract the labels\n",
    "labels = np.array(df.pop('label'))\n",
    "\n",
    "# 30% examples in test data\n",
    "train, test, train_labels, test_labels = train_test_split(df, labels, \n",
    "                                                          stratify = labels,\n",
    "                                                          test_size = 0.4, \n",
    "                                                          random_state = RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of Missing values\n",
    "\n",
    "We'll fill in the missing values with the mean of the column. It's important to note that we fill in missing values in the test set with the mean of columns in the training data. This is necessary because if we get new data, we'll have to use the training data to fill in any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(test.mean())\n",
    "\n",
    "# Features for feature importances\n",
    "features = list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll train the decision tree on the data. Let's leave the depth unlimited and see if we get overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tree\n",
    "tree.fit(train, train_labels)\n",
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Decision Tree Performance\n",
    "\n",
    "Given the number of nodes in our decision tree and the maximum depth, we expect it has overfit to the training data. This means it will do much better on the training data than on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make probability predictions\n",
    "train_probs = tree.predict_proba(train)[:, 1]\n",
    "probs = tree.predict_proba(test)[:, 1]\n",
    "\n",
    "train_predictions = tree.predict(train)\n",
    "predictions = tree.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(train_labels, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(test_labels, probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Baseline ROC AUC: {roc_auc_score(test_labels, [1 for _ in range(len(test_labels))])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model does outperform a baseline guess, but we can see it has severely overfit to the training data, acheiving perfect ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Decision Tree\n",
    "\n",
    "We'll write a short function that calculates a number of metrics for the baseline (guessing the most common label in the training data), the testing predictions, and the training predictions. The function also plots the ROC curve where a better model is to the left and towards the top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(probs))\n",
    "print(Counter(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(predictions, probs, train_predictions, train_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we can see the problem with a single decision tree where the maximum depth is not limited: __severe overfitting to the training data__. \n",
    "\n",
    "Another method to inspect the performance of a classification model is by making a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predictions)\n",
    "plot_confusion_matrix(cm, classes = ['Poor Health', 'Good Health'],\n",
    "                      title = 'Health Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the classifications predicted by the model on the test data along with the real labels. We can see that our model has many false negatives (predicted good health but actually poor health) and false positives (predicted poor health but actually good health). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "\n",
    "Finally, we can take a look at the features considered most important by the Decision Tree. The values are computed by summing the reduction in Gini Impurity over all of the nodes of the tree in which the feature is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': features,\n",
    "                   'importance': tree.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go through the definitions here, but you can look through the [data dictionary](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system#2015_formats.json) to determine the meaning of each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Full Tree\n",
    "\n",
    "As before, we can look at the decision tree on the data. This time, we have to limit the maximum depth otherwise the tree will be too large and cannot be converted and displayed as an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tree as dot file\n",
    "export_graphviz(tree, 'tree_real_data.dot', rounded = True, \n",
    "                feature_names = features, max_depth = 6,\n",
    "                class_names = ['poor health', 'good health'], filled = True)\n",
    "\n",
    "# Convert to png\n",
    "call(['dot', '-Tpng', 'tree_real_data.dot', '-o', 'tree_real_data.png', '-Gdpi=200'])\n",
    "\n",
    "# Visualize\n",
    "Image(filename='tree_real_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model is extremely deep and has many nodes. To reduce the variance of our model, we could limit the maximum depth or the number of leaf nodes. Another method to reduce the variance is to use more trees, each one trained on a random sampling of the observations. This is where the random forest comes into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "Now we can move on to a more powerful model, the random forest. This takes the idea of a single decision tree, and creates an _ensemble_ model out of hundreds or thousands of trees to reduce the variance. Each tree is trained on a random set of the observations, and for each split of a node, only a subset of the features are used for making a split. When making predictions, the random forest averages the predictions for each of the individual decision trees for each data point in order to arrive at a final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training a random forest in extremely easy in Scikit-Learn. The cell below is all you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=RSEED, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "model.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many nodes there are for each tree on average and the maximum depth of each tree. There were 100 trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each decision tree in the forest has many nodes and is extremely deep. However, even though each individual decision tree may overfit to a particular subset of the training data, the idea is that the overall random forest should have a reduced variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_predictions = model.predict(train)\n",
    "train_rf_probs = model.predict_proba(train)[:, 1]\n",
    "\n",
    "rf_predictions = model.predict(test)\n",
    "rf_probs = model.predict_proba(test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still achieves perfect measures on the training data, but this time, the testing scores are much better. If we compare the ROC AUC, we see that the random forest does significantly better than a single decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, rf_predictions)\n",
    "plot_confusion_matrix(cm, classes = ['Poor Health', 'Good Health'],\n",
    "                      title = 'Health Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the single decision tree, the model has fewer false postives although more false negatives. __Overall, the random forest does significantly better than a single decision tree__. This is what we expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                   'importance': model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi_model.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model does pretty well! Compared to the single decision tree, the random forest is much better able to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Optimization through Random Search\n",
    "\n",
    "In order to maximize the performance of the random forest, we can perform a random search for better hyperparameters. This will randomly select combinations of hyperparameters from a grid, evaluate them using cross validation on the training data, and return the values that perform the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = RSEED)\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=RSEED)\n",
    "\n",
    "# Fit \n",
    "rs.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best hyperparameter values are not the defaults. This shows the importance of tuning a model for a specific dataset. Each dataset will have different characteristics, and the model that does best on one dataset will not necessarily do the best across all datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Best Model\n",
    "\n",
    "Now we can take the best model (it has already been trained) and evaluate it. Hopefully it does better than the stock Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_predictions = best_model.predict(train)\n",
    "train_rf_probs = best_model.predict_proba(train)[:, 1]\n",
    "\n",
    "rf_predictions = best_model.predict(test)\n",
    "rf_probs = best_model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in best_model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best maximum depth is not unlimited as we see above! This indicates that restricting the maximum depth of the individual decision trees can improve the cross validation performance of the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model achieves around the same performance as the default model. More random search iterations could improve performance, or it's possible that we are close the limit of what the random forest can achieve for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_model.estimators_[1]\n",
    "\n",
    "# Export a tree from the forest\n",
    "export_graphviz(estimator, 'tree_from_optimized_forest.dot', rounded = True, \n",
    "                feature_names=train.columns, max_depth = 8, \n",
    "                class_names = ['poverty', 'no poverty'], filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call(['dot', '-Tpng', 'tree_from_optimized_forest.dot', '-o', 'tree_from_optimized_forest.png', '-Gdpi=200'])\n",
    "Image('tree_from_optimized_forest.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree is a little simpler than the solitary decision tree model. Random search found that limiting the maximum depth of the trees in the forest delivers better performance than letting them expand as far as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we built and used a random forest machine learning model in Python. Rather than just writing the code and not understanding the model, we formed an understanding of the random forest by inspecting an individual decision tree and discussion its limitations. We visualized the decision tree to see how it makes decisions and also saw how one decision tree overfits to the trainig data. __To overcome the limitations of a single decision tree, we can train hundreds or thousands of them in a single ensemble model. This model, known as a random forest, trains each tree on a different set of the training observations, and make splits at each node based on a subset of the features leading to a model with reduced variance and better generalization performance on the testing set.__\n",
    "\n",
    "A few key concepts to take away are\n",
    "\n",
    "1. Individual decision tree: intuitive model that makes decisions based on a flowchart of questions asked about feature values. Has high variance indicated by overfitting to the training data.\n",
    "2. Gini Impurity: Measure that the decision tree tries to minimize when splitting each node. Represents the probability that a randomly selected sample from a node will be incorreclty classified according to the distribution of samples in the node.\n",
    "3. Bootstrapping: sampling random sets of observations with replacement. Method used by the random forest for training each decision tree.\n",
    "4. Random subsets of features: selecting a random set of the features when considering how to split each node in a decision tree.\n",
    "5. Random Forest: ensemble model made of hundreds or thousands of decision trees using bootstrapping, random subsets of features, and average voting to make predictions. \n",
    "6. Bias-variance tradeoff: the fundamental issue in machine learning that describes the tradeoff between a model with high complexity that learns the training data very well  at the cost of not being able to generalize to the testing data (high variance), and a simple model (high bias) that cannot even learn the training data. A random forest reduces the variance of a single decision tree while also accurately learning the training data leading to better predictions on the testing data.\n",
    "\n",
    "Hopefully this notebook has given you not only the code required to use a random forest, but also the background necessary to understand how the model is making decisions. Machine learning is a powerful tool and it's important to not only know how to use the tool, but also to understand how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
